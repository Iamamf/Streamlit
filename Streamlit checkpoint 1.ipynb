{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a58795b-cc41-4138-ac77-37b6fd2bec35",
   "metadata": {},
   "source": [
    "##### In this checkpoint, we are going to work on the 'Expresso churn' dataset that was provided as part of Expresso Churn Prediction Challenge hosted by Zindi platform.\n",
    "\n",
    "### Dataset description: \n",
    "Expresso is an African telecommunications services company that provides telecommunication services in two African markets: Mauritania and Senegal. The data describes 2.5 million Expresso clients with more than 15 behaviour variables in order to predict the clients' churn probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4c32a7-0306-4ec6-848a-0a9750c6476e",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "\n",
    "- Install the necessary packages\n",
    "- Import you data and perform basic data exploration phase\n",
    "- Display general information about the dataset\n",
    "- Create a pandas profiling reports to gain insights into the dataset\n",
    "- Handle Missing and corrupted values\n",
    "- Remove duplicates, if they exist\n",
    "- Handle outliers, if they exist\n",
    "- Encode categorical features\n",
    "- Based on the previous data exploration train and test a machine learning classifier\n",
    "- Create a streamlit application (locally)\n",
    "- Add input fields for your features and a validation button at the end of the form\n",
    "- Import your ML model into the streamlit application and start making predictions given the provided features values\n",
    "\n",
    "### Note: \n",
    "\n",
    "- This checkpoint should be done locally, (Google colab notebooks won't work)\n",
    "- Make sure to run the app using the \"streamlit run\" command in your terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ea26b2-930d-4b12-b3b7-cab0851c1ebb",
   "metadata": {},
   "source": [
    "## Variable Definitions\n",
    "\n",
    "- user_id\t\t\t\t\n",
    "- REGION\t\tthe location of each client\t\t\n",
    "- TENURE\t\tduration in the network\t\t\n",
    "- MONTANT\t\ttop-up amount\t\t\n",
    "- FREQUENCE_RECH\t\tÂ number of times the customer refilled\t\t\n",
    "- REVENUE\t\tmonthly income of each client\t\t\n",
    "- ARPU_SEGMENT\t\tincome over 90 days / 3\t\t\n",
    "- FREQUENCE\t\tnumber of times the client has made an income\t\t\n",
    "- DATA_VOLUME\t\tnumber of connections\t\t\n",
    "- ON_NET\t\tinter expresso call\t\t\n",
    "- ORANGE\t\tcall to orange\t\t\n",
    "- TIGO\t\tcall to Tigo\t\t\n",
    "- ZONE1\t\tcall to zones1\t\t\n",
    "- ZONE2\t\tcall to zones2\t\t\n",
    "- MRG\t\ta client who is going\t\t\n",
    "- REGULARITY\t\tnumber of times the client is active for 90 days\t\t\n",
    "- TOP_PACK\t\tthe most active packs\t\t\n",
    "- FREQ_TOP_PACK\t\tnumber of times the client has activated the top pack packages\t\t\n",
    "- CHURN\t\tvariable to predict - Target\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a1e91d-5ea9-4327-a6a8-31639bb05028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d17d81-a2f9-43c7-bdb9-472b13d062e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#import standard visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import machine learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import xgboost\n",
    "\n",
    "from sklearn.model_selection import train_test_split #split\n",
    "from sklearn.metrics import accuracy_score #metrics\n",
    "\n",
    "#tools for hyperparameters search\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# import the label Encoder library \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6860b9e-b87e-46bf-9a1a-36753de2ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Expresso_churn_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0425e-e1a0-42af-bc6e-416f6e6be8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c01d0c3-d6f6-4dcd-87ad-115af04d420f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7d733-a395-43e2-9c06-5dc8598b4c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc7b09-5614-434d-ba8a-14e933612c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop irrelevant tables\n",
    "df = df.drop(\"user_id\", axis=1)\n",
    "df = df.drop(\"MRG\", axis=1)\n",
    "df = df.drop(\"ZONE1\", axis=1)\n",
    "df = df.drop(\"ZONE2\", axis=1)\n",
    "df = df.drop(\"DATA_VOLUME\", axis=1)\n",
    "df = df.drop(\"ON_NET\", axis=1)\n",
    "df = df.drop(\"ORANGE\", axis=1)\n",
    "df = df.drop(\"TIGO\", axis=1)\n",
    "df = df.drop(\"REGION\", axis=1)\n",
    "df = df.drop(\"TOP_PACK\", axis=1)\n",
    "df = df.drop(\"FREQ_TOP_PACK\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e88175-8391-45c2-989a-fd8c432d0bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['MONTANT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c0797-69e3-4bed-b563-1d96c61df4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['REVENUE'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aeb4caf-bf78-4f98-b36f-1334a53f370e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"FREQ_TOP_PACK\"].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51bbd0-287c-478f-b1a0-3b1f5a215b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include = 'object').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9b948-ad10-4959-96bb-f13e3fd44dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TENURE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6be3aaa-590d-4d7e-a19e-8271de2ddf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add598f2-c63f-4979-802e-f16e3bdc5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['TENURE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23ce89-4946-4206-be5e-11029c327adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\n",
    "     'TENURE_D 3-6 month': 'Between_3to6',\n",
    "     'TENURE_E 6-9 month': 'Between_6to9',\n",
    "     'TENURE_F 9-12 month': 'Between_9to12',\n",
    "     'TENURE_G 12-15 month': 'Between_12to15',\n",
    "     'TENURE_H 15-18 month': 'Between_15to18',\n",
    "     'TENURE_I 18-21 month': 'Between_18to21',\n",
    "     'TENURE_J 21-24 month': 'Between_21to24',\n",
    "     'TENURE_K > 24 month': 'More_than_24'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7578583-2053-4e4c-9501-41466cf8bf80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"Expresso_churn_dataset_cleaned.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e12c0e-c04d-4ede-bd56-37bcfba7539c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a228e-f72e-4719-92ae-cfb7f9234ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = df.select_dtypes(include='number').columns\n",
    "numerical_features\n",
    "plt.figure(figsize=(15, 7.5))\n",
    "correlation_matrix = df[numerical_features].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e522e3-6339-42ca-b0e4-fb5504c4f2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"Expresso_churn_dataset_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23b4188-aec5-4615-9e47-8046daf33b7e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f101ff90-b99a-49a8-8c06-577199f0d43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming limited_df is your DataFrame containing the features and target variable\n",
    "X, y = df.drop(\"CHURN\", axis=1), df[\"CHURN\"]  # Replace 'target' with your actual column name\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize classification models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "    \"Ridge Classifier\": RidgeClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest Classifier\": RandomForestClassifier(),\n",
    "    \"Decision Tree Classifier\": DecisionTreeClassifier(),\n",
    "    \"XgBoost\": xgboost.XGBClassifier()\n",
    "}\n",
    "\n",
    "best_model = None\n",
    "best_f1 = float(\"-inf\")\n",
    "\n",
    "# Train each model and evaluate its performance\n",
    "for name, model in models.items():\n",
    "    # Fit the model to the training data\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate the model using accuracy, precision, recall, and F1-score\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    print(f\"{name}:\\n Accuracy: {accuracy:.4f}\\n Precision: {precision:.4f}\\n Recall: {recall:.4f}\\n F1-score: {f1:.4f}\\n\")\n",
    "    \n",
    "    # Select the best model based on F1-score\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_model = name\n",
    "\n",
    "print(f\"\\nBest Model: {best_model}\\n Best F1-score: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e49c3b-c267-445d-9805-46cdf097a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"churn_analysis.py\", \"w\") as file:\n",
    "    file.write('''\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"Downloads/Expresso_churn_dataset_cleaned.csv\")\n",
    "\n",
    "# Assuming 'target' column contains the labels for whether a person has a bank account or not\n",
    "X = df.drop(columns=['CHURN'])  # Features\n",
    "Y = df['CHURN']  # Target labels\n",
    "\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Train the classifier\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=4,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=100,\n",
    "    subsample=1.0\n",
    ")\n",
    "gb_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Streamlit app title\n",
    "st.title(\"Customer Churn Prediction\")\n",
    "\n",
    "# App description\n",
    "st.write(\"\"\"\n",
    "This app uses **eXtreame Gradient Boosting Classifier** to predict which individuals are most likely to churn from the Service Provider.\n",
    "\"\"\")\n",
    "\n",
    "# Display the dataset\n",
    "st.write(\"### Xpresso Dataset Sample\", df.head())\n",
    "\n",
    "# Sidebar for user inputs\n",
    "st.sidebar.header('User Input Parameters')\n",
    "\n",
    "def user_input_features():\n",
    "    MONTANT = st.sidebar.slider('Recharge Amount', int(df['MONTANT'].min()), int(df['MONTANT'].max()), int(df['MONTANT'].mean()))\n",
    "    FREQUENCE_RECH = st.sidebar.slider('Recharge Frequency', int(df['FREQUENCE_RECH'].min()), int(df['FREQUENCE_RECH'].max()), int(df['FREQUENCE_RECH'].mean()))\n",
    "    REVENUE = st.sidebar.slider('Revenue', int(df['REVENUE'].min()), int(df['REVENUE'].max()), int(df['REVENUE'].mean()))\n",
    "    ARPU_SEGMENT = st.sidebar.slider('income over 90days/3', int(df['ARPU_SEGMENT'].min()), int(df['ARPU_SEGMENT'].max()), int(df['ARPU_SEGMENT'].mean()))\n",
    "    FREQUENCE = st.sidebar.slider('Client monthly income', int(df['FREQUENCE'].min()), int(df['FREQUENCE'].max()), int(df['FREQUENCE'].mean()))\n",
    "    REGULARITY = st.sidebar.slider('Regularity', int(df['REGULARITY'].min()), int(df['REGULARITY'].max()), int(df['REGULARITY'].mean()))\n",
    "    Duration_of_use = st.sidebar.selectbox('Duration_of_use', ['Between_3to6', 'Between_6to9', 'Between_9to12', 'Between_12to15', 'Between_15to18', 'Between_18to21', 'Between_21to24', 'More_than_24'])\n",
    "    \n",
    "    data = {\n",
    "        'MONTANT': MONTANT,\n",
    "        'FREQUENCE_RECH': FREQUENCE_RECH,\n",
    "        'REVENUE': REVENUE,\n",
    "        'ARPU_SEGMENT': ARPU_SEGMENT,\n",
    "        'FREQUENCE': FREQUENCE,\n",
    "        'REGULARITY': REGULARITY,\n",
    "        'Duration_of_use': Duration_of_use\n",
    "    }\n",
    "\n",
    "    # Convert the input data into a DataFrame\n",
    "    features = pd.DataFrame(data, index=[0])\n",
    "    \n",
    "    # Align the features with the training data (adding missing columns if necessary)\n",
    "    features = features.reindex(columns=X_train.columns, fill_value=0)\n",
    "    \n",
    "    return features\n",
    "\n",
    "input_df = user_input_features()\n",
    "\n",
    "# Make predictions based on user input\n",
    "prediction = gb_clf.predict(input_df)\n",
    "prediction_proba = gb_clf.predict_proba(input_df)\n",
    "\n",
    "# Display the prediction results\n",
    "st.subheader('Prediction')\n",
    "st.write(f\"Predicted Bank Account Status: {'Will churn' if prediction[0] == 1 else 'Will not churn'}\")\n",
    "\n",
    "st.subheader('Prediction Probability')\n",
    "st.write(prediction_proba)\n",
    "\n",
    "# Feature importance visualization\n",
    "st.subheader('Feature Importance')\n",
    "importance = gb_clf.feature_importances_\n",
    "features = X.columns\n",
    "plt.barh(features, importance)\n",
    "st.pyplot(plt)\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4cbdc-ca66-44aa-a737-b3dff7e84d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e9017f-50d2-4a53-9794-3daf661432d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
